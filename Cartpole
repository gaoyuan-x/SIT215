##reference from Mofan 
##https://blog.csdn.net/gg_18826075157/article/details/78163386

import gym
import numpy as np

env = gym.make('CartPole-v0')

max_number_of_steps = 200   # Highest score per game
#---------The winning condition is that the average score of the last 100 games is higher than 195-------------
goal_average_steps = 195
num_consecutive_iterations = 100
#----------------------------------------------------------
num_episodes = 5000 # 5000 games in total
last_time_steps = np.zeros(num_consecutive_iterations)  # Only the last 100 games are stored (it can be understood that it is a stack with 100 capacity)

# q_ Table is a 256 * 2 two-dimensional array
# The discretized states have 4 ^ 4 = 256 possible values, and each state will correspond to an action
# q_ Table [S] [a] is the favorable evaluation value of action a when the state is s
# What our AI model needs to train and learn is this mapping table
q_table = np.random.uniform(low=-1, high=1, size=(4 ** 4, env.action_space.n))

# Box processing function, and [clip "_ min,clip_ The max] interval is divided into num segment, and the characteristic value x in the I section will be discretized into I
def bins(clip_min, clip_max, num):
    return np.linspace(clip_min, clip_max, num + 1)[1:-1]

# Discrete processing, the state vector composed of four continuous eigenvalues is transformed into an integer discrete value of 0 ~ ~ 255
def digitize_state(observation):
    # Scatter the vector back to four continuous eigenvalues
    cart_pos, cart_v, pole_angle, pole_v = observation
    # Each continuous eigenvalue is discretized (divided into boxes)
    digitized = [np.digitize(cart_pos, bins=bins(-2.4, 2.4, 4)),
                 np.digitize(cart_v, bins=bins(-3.0, 3.0, 4)),
                 np.digitize(pole_angle, bins=bins(-0.5, 0.5, 4)),
                 np.digitize(pole_v, bins=bins(-2.0, 2.0, 4))]
    # Four discrete values are combined into one discrete value as the final result
    return sum([x * (4 ** i) for i, x in enumerate(digitized)])

# According to this action and its feedback (the status of the next time step), return to the next best action
def get_action(state, action, observation, reward, episode):
    next_state = digitize_state(observation)
    epsilon = 0.5 * (0.99 ** episode)  # ε- Some problems in greedy strategy ε
    if epsilon <= np.random.uniform(0, 1):
        next_action = np.argmax(q_table[next_state])
    else:
        next_action = np.random.choice([0, 1])
    #-------------------------------------Training and learning, updating q_Table----------------------------------
    alpha = 0.2     # Learning coefficient α
    gamma = 0.99    # Compensation attenuation coefficient γ
    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * q_table[next_state, next_action])
    # -------------------------------------------------------------------------------------------
    return next_action, next_state

# Repeat one game after another
for episode in range(num_episodes):
    observation = env.reset()   # Initialize the environment of the game
    state = digitize_state(observation)     # Get initial state value
    action = np.argmax(q_table[state])      # Make action decision according to state value
    episode_reward = 0
    # A game is divided into time steps
    for t in range(max_number_of_steps):
        env.render()    # Update and render the game
        observation, reward, done, info = env.step(action)  # Get feedback on this action
        # Punish the fatal mistake action with great strength, and let the model learn from it
        if done:
            reward = -200
        action, state = get_action(state, action, observation, reward, episode)  # Make the decision for the next action
        episode_reward += reward
        if done:
            print('%d Episode finished after %f time steps / mean %f' % (episode, t + 1, last_time_steps.mean()))
            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))  # Update the score stack of the last 100 games
            break
            # If the average score of the last 100 games is higher than 195
        if (last_time_steps.mean() >= goal_average_steps):
            print('Episode %d train agent successfuly!' % episode)
            break

print('Failed!')
